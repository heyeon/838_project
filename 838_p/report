\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{endnotes, hyperref, geometry,calc,epstopdf,usenix, graphicx, float, multicol}
\date{} 
\title{
\Large 
\bf Reducing Data Center Latency
}

\author{ 
{\rm Zainab Ghadiyali}\\ 
\textit{zainab@cs.wisc.edu}
\and {\rm Michael Griepentrog}\\
\textit{mgriepentrog@cs.wisc.edu}\\ 
\and{\rm Aditya Akella}\\
\textit{akella@cs.wisc.edu}
} 

\begin{document}
\maketitle

\thispagestyle{empty}
\subsection*{Abstract}
Latency is usually compromised in order to improve bandwidth. However, 
with the advent of latency sensitive applications such as high frequency trading
, the focus on reducing latency in network has increased. Furthermore, computation is 
increasingly also shifting to datacenters. Coupled together, these two factors
necessitate further study on latency in datacenters. In this body of work, we
study various factors that may influence latency, such as type of flows,
offloading parameters, hardware such as network interface cards, optimized
algorithms such as stochastic fair queueing and type of routers (presence and
absence of switches). By utilizing a server-client architecture and OpenFlow
switches, we show that offloading, stochastic fair queueing and utilization
switches over direct flows help reduce latency in networks. Further, we quantify
the difference and propose further experiments that would help identify other
latency influencing factors. 

\section{Introduction} 
In the last few decades, the key focus of the network
community has been on improving the overall goodput of networks. The initial
focus on circuit switching moved to packet switching due to bandwidth and
hardware inefficiencies in working with network resources for bursty
communication traffic. The Transmission Control Protocol(TCP) was invented to
address the issue of bandwidth/congestion collapse in the network and ensure
bandwidth fairness.~\cite{dctcp,queueing,tcpPacing} \\However network latency has
been given less importance. The last 30 decades have brought a meager 30x
reduction in latency, versus the 3000x times improvement in bandwidth. Several
studies have been conducted in the 90s to tackle the latency issue, but they
were not adopted. ~\cite{chiola, chun, eicken, dittia} Additionally, most
applications were throughput oriented (e.g.  email) and hence not sensitive of
delivery time. For example, switches are especially designed with large packet
buffers in order to avoid congestion since TCP protocol does not deal well with
packet drops from congestion. Network Interface Cards (NICs) are optimized to
delay interrupts for up to 30 $\mu$s. Latency for latency sensitive applications
is seen as an acceptable trade-off in order to maintain high bandwidth
utilization.  \\However, there is now an increasing interest in reducing latency
in data centers. A substantial amount of computing is now shifting to data
centers and reducing latency is now easier given the confines of a building
rather than tackling the issue for the internet at large.  Furthermore, ultra
low latency sensitive applications such as High Frequency Trading, HPC and
Google Instant Search service could especially benefit from this.  Several
high-frequency traders implement trading applications with a goal to reach a
trade decision in under 100 microseconds. NIC, end-host stacks and switches are
all points where a packet may experience latency while traversing from client to
server.  In order to address delays arising from queueing delay at switches,
DCTCP ~\cite{dctcp} uses ECN marking to slow down flows before the queue is
saturated. HULL ~\cite{hull} further suggests trading off a little bandwidth to
provide smaller amount of latency.  In this paper, we look into understanding
how switch, various offloading parameters and hardware influences latency.

OUR OBSERVATION IS THAT

THROUGH OUR EVALUATION WE FIND THAT

\section{Methodology} 
\subsection{Design} 
A strong evaluation and understanding of reducing latency will be incomplete
without simultaneously studying the trade-offs between latency and other
important factors such as bandwidth and CPU
utilization. Thus, we study latency along with its influence on CPU utilization
and bandwidth. Furthermore, it would be important to also understand the role of
latency influencing factors in UDP and TCP. UDP should be faster than TCP since
it allows a continuous packet stream versus TCP that sends acknowledgements
(ACKS) for a set of packets calculated using the TCP window size and round trip
time (RTT) Thus, metrics were obtained for TCP and UDP flows.  Offloading moves
the IP and TCP processing to the NICs. Keeping type of NIC constant, we can
study the influence of offloading on latency. TCP segmentation offload (TSO)/
Generic segmentation offload (GSO) are considered useful in increase outbound
throughput of high-bandwidth network connections since it reduces host CPU
cycles for protocol header processing and checksumming.Generic receiving offload
(GRO) attempts to replicate the TSO modus operandi on the receiver-side. Effects
of offloading is studied by:

\begin{enumerate} 
\item TSO/GSO and GRO : On 
\item TSO/GSO and GRO : Off 
\item TSO/GSO : On and GRO : Off 
\item TSO/GSO : Off and GRO : On 
\end{enumerate} 

\subsection{Experimental Setup} 
Two commodity computers connected via an
OpenFlow switch ~\cite{openflow}. One of the computers served as a client and
the other as server. Eight experiments were ran in this setting. Another eight
experiments ran on the two machines in absence of OpenFlow switch. For each
experiment, throughput is recorded using iperf, cpu utilization through pidstat,
interpacket delay through tcpdump and latency through ping. This process was
automated by running bash scripts on server and client end. Offloading
parameters were varied using ethtool. Data was analysed using wireshark and a
python script that parsed results from experiments and provided basic summary in
terms of min, max, stdev and average latency, average throughput in terms of
Mbits/sec and average percent CPU utilization.  

/subsubsection{Testing throughput}
Iperf is known to be an easy to use and valuable tool for performing network
throughput measurements ~\cite{iperf}. A server and client are established and
packets are transferred for 60 seconds at an interval of 10 seconds. For UDP
data streams, a bandwidth of 511 Mbits/sec was specified.  
/subsubsection{Testing CPU utilization}
\section{Evaluation}
\subsection{Bandwidth} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/cpuTcp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization}
\label{fig:cpuTcp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/latencyTCP.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency}
\label{fig:latencyTcp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/latencyUDP.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency}
\label{fig:latencyUdp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/cpuClientTcpUdp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization in presence of switch on client end}
\label{fig:cpuClientTcpUdp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/cpuClientTcpUdpNoSwitch.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization in absence of switch on client end}
\label{fig:cpuClientTcpUdpNoSwitch}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/cpuServerTcpUdp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization in presence of switch on server end}
\label{fig:cpuServerTcpUdp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/cpuServerTcpUdpNoSwitch.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization in absence of switch on server end}
\label{fig:cpuServerTcpUdpNoSwitch}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/latencyFifo.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency for FIFO algorithm}
\label{fig:latencyFIFO}
\end{figure}

These results in Figure ~\ref{fig:latencyFIFO} would indicate that there were
larger buffers at end-hosts than the switch. While there aren't any data center
topologies that connect end-hosts
directly together, this illustrates the importance of sizing the buffers
correctly. Having all offloading reduces latency. Avg UDP latency w/ switch is
.29-.3ms. UDP latency w/o switch is .191-.234ms. Roughly .1ms for a RTT.

TSO works by moving work of breaking down large chunks of data over to the NIC.
In Figure ~\ref{fig:bandwidthClientNoSwitch}, the x-axis represents offloading settings
while y-axis represents bandwidth. Bandwidth is almost halved when TSO/GSO if
switched off. This suggests that throughput may be increased by
switching off offloading. Keeping GRO on in the absence of TSO/GSO did
not show an appreciable change, thus suggested that GRO plays a little
to no role in increasing bandwidth. Interestingly, the bandwidth
remained fairly constant for switch as well as client despite varying
offloading parameters when flows traversed through an OpenFlow switch. 


In Figure ~\ref{fig:cpuClientNoSwitch}, x-axis represents the offloading parameters,
while y-axis respresents the average percent CPU utilization for TCP and UDP
flows for a client when flows directly traverse from client to server.
Irrespective of offloading settings, 100\% CPU utilization is observed
for UDP flows. This makes sense, since no offloading should occur for
UDP flows.  However, we see almost 100\% CPU utilization for TCP
flows in the absence of TSO/GSO. This is in line with findings
expressed in Figure ~\ref{fig:bandwidthClientNoSwitch}. Since the NIC non
longer performs offloading, CPU is utilized for segmentation.
The CPU utilization increases 10x with a 2x decrease in
bandwidth.  In ~\ref{fig:cpuClientSwitch}, x-axis represents the
offloading parameters, while y-axis represents the average
percent CPU utilization for TCP and UDP flows when a switch is
placed in server-client path. As observed in
~\ref{fig:cpuClientNoSwitch}, the average CPU utilization
remains unaffected by varying offloading parameters for UDP
flows. However, for TCP flows, a significantly uniform and small
percent of CPU utilization is observed. 

In ~\ref{fig:cpuServerNoSwitch}, the x-axis represents the offloading parameters,
while y-axis represents the average percent cpu utilization.The fairly even
distribution of cpu percent utilization with or without switch suggests
that packet segmentation does not occur at the server end.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/latencySFQ.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency for SFQ algorithm}
\label{fig:latencySFQ}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/latencySFQTcp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency for SFQ algorithm with TCP flows} 
\label{fig:latencySFQTcp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/latencySFQUdp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency for SFQ algorithm with UDP flows} 
\label{fig:latencySFQUdp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/bandwidthClientNoSwitch.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average bandwidth in mbits/sec on client end in absence of switch} 
\label{fig:bandwidthClientNoSwitch}
\end{figure}
\subsection{Latency} In figavglatencyfifosfq, the x-axis represents the
offloading parameters, while y-axis represents the latency in ms for TCP flows
at the client end. A distinct difference in latency is observed for when GRO is
switched off for SQF.  In figavglatencyFIFO, we see the average latency in ms
for UDP flows is independent of offloading settings. However, for TCP,
        TSO/GSO's presence dramatically increases latency in absence of switch. 

While studying the latency profile for UDP flows, we notice a significantly
higher latency in presence of TSO\/GSO for without a switch case. However, not
sure how this makes sense.

\section{Limitations and Future Work} 

\emph{Remore Direct Memory Access (RDMA)} NICs would be interesting to study
since it simplifies many problems observed with generic TCP offloading. It would
be especially interesting and important to relate latency observed from
utilizing RDMA and CPU utilization on non-RDMA based NICs. Since RDMAs enable
the network adapter to transfer data directly to or from application memory, no
work is required from CPUs, caches or context switches.

\emph{Maximum Transmission Unit (MTU)} is the greatest amount of data that can
be transferred in one physical frame over the network. Thus, increasing MTU, may
decrease latency. The standard MTU for etnernet is 1500 bytes. We would have
like to increase that upto 9000 units, but were unable to perform experiments
due to hardware limitations.  

\section{Conclusion}
In this paper we study types of flows, influence of switches and varying
offloading parameters on TCP. We show that optimizing for buffer sizes is
important to achieve desired goals. Offloading helps reduce latency in TCP
flows, while UDP flows remain largely uninfluenced.  
Throughput remain mostly unchanged for TCP, except in the no switch case.
Throughput dropped when TSO/GSO is disabled. This is likely due to 100% CPU
utilization, but it's unclear in what's different between the switch/no switch
case. SFQ provides lower average latency, but higher variation. 15ms stdev when all
settings are enabled. Increases in CPU utilization seem minimal/non existent.
These 
\section{Acknowledgments}

We would like to thank Aaron Gember without whose help, running experiments on
the Open Flow test best at the University of Wisconsin-Madison would not be
possible.  


{\footnotesize \bibliography{mybib} {} \bibliographystyle{acm} 


}

\theendnotes

\end{document}
