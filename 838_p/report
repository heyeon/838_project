\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage[left=2.5cm]{geometry}
\usepackage{endnotes, hyperref, geometry,calc,epstopdf,usenix, graphicx, float}
\date{} 
\title{
\Large 
\bf Reducing Data Center Latency
}

\author{ 
{\rm Zainab Ghadiyali}\\ 
\textit{zainab@cs.wisc.edu}
\and {\rm Michael Griepentrog}\\
\textit{griepent@cs.wisc.edu}\\ 
\and{\rm Aditya Akella}\\
\textit{akella@cs.wisc.edu}
} 

\begin{document}
\maketitle

\thispagestyle{empty}
\subsection*{Abstract}
Latency is usually compromised in order to improve bandwidth. However, 
with the advent of latency sensitive applications such as high frequency trading
, the focus on reducing latency in network has increased. Furthermore,
computation is increasingly also shifting to datacenters. Coupled together,
these two factors necessitate further study on latency in datacenters. In this
body of work, we present a measurement study where we examine various factors
such as CPU utilization, send and receive offloading, and how queueing affects
latency. %By utilizing a server-client architecture and OpenFlow switches, we
show that offloading, stochastic fair queueing and utilization
%switches over direct flows help reduce latency in networks. Further, we
%quantify the difference and propose further experiments that would help
%identify other latency influencing factors. 

\section{Introduction} 
In the last few decades, the key focus of the network
community has been on improving the overall goodput of networks. The initial
focus on circuit switching moved to packet switching due to bandwidth and
hardware inefficiencies in working with network resources for bursty
communication traffic. The Transmission Control Protocol(TCP) was invented to
address the issue of bandwidth/congestion collapse in the network and ensure
bandwidth fairness.~\cite{dctcp,queueing,tcpPacing} However network latency has
been given less importance. The last 30 decades have brought a meager 30x
reduction in latency, versus the 3000x times improvement in bandwidth. Several
studies have been conducted in the 90s to tackle the latency issue, but they
were not adopted. ~\cite{chiola, chun, eicken, dittia} Additionally, most
applications were throughput oriented (e.g.  email) and hence not sensitive of
delivery time. For example, switches are especially designed with large packet
buffers in order to avoid congestion since TCP protocol does not deal well with
packet drops from congestion. Network Interface Cards (NICs) are optimized to
delay interrupts for up to 30 $\mu$s. Latency for latency sensitive applications
is seen as an acceptable trade-off in order to maintain high bandwidth
utilization.  \\However, there is now an increasing interest in reducing latency
in datacenters. A substantial amount of computing is now shifting to data
centers and examining ways of reducing latency in data centers is interesting 
as the data center operator has control of all aspects of the network. However, 
it is still important to investigate end-host solutions for cases where a tenant 
is co-locating servers in a data center and does not control the network topology 
or switches. Reducing latency is especially important in applications such as
High Frequency Trading, HPC and Google Instant Search service could especially
benefit from this.  Several
high-frequency traders implement trading applications with a goal to reach a
trade decision in under 100 microseconds. NIC, end-host stacks and switches are
all points where a packet may experience latency while traversing from client to
server.  In order to address delays arising from queueing delay at switches,
DCTCP ~\cite{dctcp} uses ECN marking to slow down flows before the queue is
saturated. HULL ~\cite{hull} further suggests trading off a little bandwidth to
provide smaller amount of latency by never allowing queues to build up at the
switch.  In this paper, we aim to understand how offloading, switches, NICs, and
queues affect latency, and what are the potential trade-offs.

We observe that offload parameters positive influence latency. That is, latency
decreases in the presence of offloading. Queueing algorithms such as stochastic
fair queueing may also help reduce latency, although our data also suggests high
standard deviation which necessitates further experimentation. 

\section{Methodology} 
\subsection{Design} A strong evaluation and understanding
of reducing latency will be incomplete without simultaneously studying the
trade-offs between latency and other important factors such as bandwidth and CPU
utilization. Thus, we study latency along with its influence on CPU utilization
and bandwidth. We also aim to examine the effects of queueing. In one set of 
experiments, we use TCP which by nature will cause queueing. In the other set 
of experiments, we use a 500 Mbps UDP flow to simulate some traffic on the network, 
but latency sensitive flows should not be queued. To represent a latency sensitive flow, 
we ping the other host every 200 ms for the duration of the TCP/UDP flow. Offloading moves
the IP and TCP processing to the NICs. Keeping type of NIC constant, we can
study the influence of offloading on latency. TCP segmentation offload (TSO)/
Generic segmentation offload (GSO) are considered useful in increase outbound
throughput of high-bandwidth network connections since it reduces host CPU
cycles for protocol header processing and checksumming.Generic receiving offload
(GRO) attempts to replicate the TSO modus operandi on the receiver-side. Table~\ref{table:offloading}
shows the configurations used at both end-hosts for each experiment.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
  \hline
  & TSO/GSO & GRO \\ \hline
  Experiment 1 & On & On \\ \hline
  Experiment 2 & Off & Off \\ \hline
  Experiment 3 & On & Off \\ \hline
  Experiment 4 & Off & On \\ \hline
\end{tabular}
\caption{Possible parameters for offloading.}
\label{table:offloading}
\end{table}

\subsection{Experimental Setup} Two commodity computers connected via an
OpenFlow switch ~\cite{openflow}. One of the computers served as a client
(sender of traffic) and the other as server (receiver of traffic). Eight
experiments were run in this setting. Another eight experiments were run with
the two machines connected directly together. For each
experiment, throughput is recorded using iperf, CPU utilization through pidstat,
inter-packet delay with tcpdump and latency through ping. This process was
automated by running bash scripts on server and client end. Offloading
parameters were varied using ethtool. Data was analysed using wireshark and a
python script that parsed results from experiments and provided basic summary in
terms of min, max, stdev and average latency, average throughput in terms of
Mbits/sec and average percent CPU utilization. These experiments were run on
multi-core machines at coarse timescales (1 second for CPU measurements) and it
is our belief that our instrumentation adds little overhead.\section{Evaluation}
\section{Evaluation}
In our evaluation, we look at the following metrics: CPU utilization, delay of the latency sensitive flow, and throughput with varying offloading parameters and with/without a switch.
\subsection{TCP} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/bandwidthClientNoSwitch.ps}
\caption{TCP Throughput with various offloading combinations without the presence of a switch at the client (sender).} 
\label{fig:bandwidthClientNoSwitch}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/cpuClientTcpUdpNoSwitch.ps}
\caption{In all cases, UDP throughput utilizes 100\% of one CPU core. This may be due to polling, as the flow is not fully utilizing the link. This not the case for TCP, as the increase in CPU utilization results in a decrease in throughput.}
\label{fig:cpuClientTcpUdpNoSwitch}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/latencyTCP.ps}
\caption{TCP latency with and without a switch. In the presence of a switch, latency predictably increases when offloading is disabled. More puzzling however, is that without a switch, latency {\it decreases}. It's unclear how dedicated hardware for segmenting does not provide lower latency than a CPU.}
\label{fig:latencyTcp}
\end{figure}

TSO works by moving the work of breaking down large chunks of data into MTU-sized packets over to the NIC.
Figure~\ref{fig:bandwidthClientNoSwitch}, illustrates the benefit of this. Bandwidth is almost halved when TSO/GSO if disabled. If we look at 
Figure~\ref{fig:cpuClientTcpUdpNoSwitch}, we see that CPU utilization increases to 100\% when offloading is disabled. This likely contributes to the 
drop in throughput seen in Figure~\ref{fig:bandwidthClientNoSwitch}. GRO seems to have no impact on throughput as the flow is asymmetric and is only receiving ACKs. Interestingly, average latency is lower when offloading is disabled (Figure ~\ref{fig:latencyTCP}) when not switch is present.


In Figure ~\ref{fig:cpuClientTcpUdpNoSwitch}, x-axis represents the offloading parameters,
while y-axis respresents the average percent CPU utilization for TCP and UDP
flows for a client when flows directly traverse from client to server.
Irrespective of offloading settings, 100\% CPU utilization is observed
for UDP flows. This makes sense, since no offloading should occur for
UDP flows.  However, we see almost 100\% CPU utilization for TCP
flows in the absence of TSO/GSO. This is in line with findings
expressed in figavgBandClientNoswitch. Since the NIC non
longer performs offloading, CPU is utilized for segmentation.
The CPU utilization increases 10x with a 2x decrease in
bandwidth.  In figavgcpuclientswitch, x-axis represents the
offloading parameters, while y-axis represents the average
percent CPU utilization for TCP and UDP flows when a switch is
placed in server-client path. As observed in
figavgcpuclientnoswitch, the average CPU utilization
remains unaffected by varying offloading parameters for UDP
flows. However, for TCP flows, a significantly uniform and small
percent of CPU utilization is observed. 

In Figure ~\ref{fig:avgcpuservernoswitch}, the x-axis represents the offloading parameters,
while y-axis represents the average percent cpu utilization.The fairly even
distribution of cpu percent utilization with or without switch suggests
that packet segmentation does not occur at the server end.

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/cpuTcp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization}
\label{fig:cpuTcp}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/latencyTCP.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency}
\label{fig:latencyTcp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/latencyUDP.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency}
\label{fig:latencyUdp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/cpuClientTcpUdp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization in presence of switch on client end}
\label{fig:cpuClientTcpUdp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/cpuClientTcpUdpNoSwitch.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization in absence of switch on client end}
\label{fig:cpuClientTcpUdpNoSwitch}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/cpuServerTcpUdp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization in presence of switch on server end}
\label{fig:cpuServerTcpUdp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/cpuServerTcpUdpNoSwitch.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average percent CPU utilization in absence of switch on server end}
\label{fig:cpuServerTcpUdpNoSwitch}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/latencyFifo.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency for FIFO algorithm}
\label{fig:latencyFIFO}
\end{figure}

These results in Figure ~\ref{fig:latencyFIFO} would indicate that there were
larger buffers at end-hosts than the switch. While there aren't any data center
topologies that connect end-hosts
directly together, this illustrates the importance of sizing the buffers
correctly. Having all offloading reduces latency. Avg UDP latency w/ switch is
.29-.3ms. UDP latency w/o switch is .191-.234ms. Roughly .1ms for a RTT.

TSO works by moving work of breaking down large chunks of data over to the NIC.
In Figure ~\ref{fig:bandwidthClientNoSwitch}, the x-axis represents offloading settings
while y-axis represents bandwidth. Bandwidth is almost halved when TSO/GSO if
switched off. This suggests that throughput may be increased by
switching off offloading. Keeping GRO on in the absence of TSO/GSO did
not show an appreciable change, thus suggested that GRO plays a little
to no role in increasing bandwidth. Interestingly, the bandwidth
remained fairly constant for switch as well as client despite varying
offloading parameters when flows traversed through an OpenFlow switch. 


In Figure ~\ref{fig:cpuClientTcpUdpNoSwitch}, x-axis represents the offloading parameters,
while y-axis represents the average percent CPU utilization for TCP and UDP
flows for a client when flows directly traverse from client to server.
Irrespective of offloading settings, 100\% CPU utilization is observed
for UDP flows. This makes sense, since no offloading should occur for
UDP flows.  However, we see almost 100\% CPU utilization for TCP
flows in the absence of TSO/GSO. This is in line with findings
expressed in Figure ~\ref{fig:bandwidthClientNoSwitch}. Since the NIC non
longer performs offloading, CPU is utilized for segmentation.
The CPU utilization increases 10x with a 2x decrease in
bandwidth.  In ~\ref{fig:cpuClientTcpUdp}, x-axis represents the
offloading parameters, while y-axis represents the average
percent CPU utilization for TCP and UDP flows when a switch is
placed in server-client path. As observed in
~\ref{fig:cpuClientTcpUdpNoSwitch}, the average CPU utilization
remains unaffected by varying offloading parameters for UDP
flows. However, for TCP flows, a significantly uniform and small
percent of CPU utilization is observed. 

In ~\ref{fig:cpuServerTcpUdpNoSwitch}, the x-axis represents the offloading parameters,
while y-axis represents the average percent cpu utilization.The fairly even
distribution of cpu percent utilization with or without switch suggests
that packet segmentation does not occur at the server end.
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/latencySFQ.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency for SFQ algorithm}
\label{fig:latencySFQ}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/latencySFQTcp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency for SFQ algorithm with TCP flows} 
\label{fig:latencySFQTcp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/latencySFQUdp.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average latency for SFQ algorithm with UDP flows} 
\label{fig:latencySFQUdp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/bandwidthClientNoSwitch.ps}
\caption{The x$-$axis represents offloading parameters while y$-$axis looks at
average bandwidth in mbits/sec on client end in absence of switch} 
\label{fig:bandwidthClientNoSwitch}
\end{figure}
\subsection{Latency} In figavglatencyfifosfq, the x-axis represents the
offloading parameters, while y-axis represents the latency in ms for TCP flows
at the client end. A distinct difference in latency is observed for when GRO is
switched off for SQF.  In figavglatencyFIFO, we see the average latency in ms
for UDP flows is independent of offloading settings. However, for TCP,
        TSO/GSO's presence dramatically increases latency in absence of switch. 

While studying the latency profile for UDP flows, we notice a significantly
higher latency in presence of TSO\/GSO for without a switch case. However, not
sure how this makes sense.

\section{Limitations and Future Work} 

\emph{Remore Direct Memory Access (RDMA)} would be interesting to study
since it simplifies many problems observed with generic TCP offloading. It would
be especially interesting and important to relate latency observed from
utilizing RDMA and CPU utilization on non-RDMA based NICs. Since RDMAs enable
the network adapter to transfer data directly to or from application memory, no
work is required from CPUs, caches or context switches.

\emph{Maximum Transmission Unit (MTU)} is the greatest amount of data that can
be transferred in one physical frame over the network. Thus, increasing MTU, may
decrease latency. The standard MTU for ethernet is 1500 bytes. We would have
like to increase that up to 9000 units, but were unable to perform experiments
due to hardware limitations.  

Although we study latency with the primary target being data centers, our work
is conducted on two multi-core machines. Multiple hops taken by a packet while
traversing from client to server would influence latency, and we are unable to
study this variable with our experimental setup. 
\emph{Source files}:
\begin {enumerate}
\item Report:"root@wings-openflow-1:~/users/zainab/838-project/report.tar.gz"
\item Scripts:"root@wings-openflow-1:~/users/zainab/838-project/report.tar.gz"
\end{enumerate}

\section{Conclusion}
In this paper we study types of flows, influence of switches and varying
offloading parameters on TCP. We show that optimizing for buffer sizes is
important to achieve desired goals. Offloading helps reduce latency in TCP
flows, while UDP flows remain largely uninfluenced.  
Throughput remain mostly unchanged for TCP, except in the no switch case.
Throughput dropped when TSO/GSO is disabled. This is likely due to 100% CPU
utilization, but it's unclear in what's different between the switch/no switch
case. SFQ provides lower average latency, but higher variation. 15ms stdev when all
settings are enabled. Increases in CPU utilization seem minimal/non existent.

\section{Acknowledgments}

We would like to thank Aaron Gember without whose help, running experiments on
the Open Flow test best at the University of Wisconsin-Madison would not be
possible.  


{\footnotesize \bibliography{mybib} {} \bibliographystyle{acm} 


}

%\theendnotes

\end{document}
